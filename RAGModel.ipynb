{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6484645-7a69-483d-ab99-441514f440c0",
   "metadata": {},
   "source": [
    "If any problems occur, please refer to the E2_Vector_Search to check out the set up process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c808834-cb99-4dee-9097-7c766734cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 17:30:07.563339: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 17:30:08.734106: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-12-05 17:30:08.734232: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-12-05 17:30:08.734245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import Vertex AI SDK\n",
    "PROJECT_ID = !gcloud config get project\n",
    "PROJECT_ID = PROJECT_ID.n\n",
    "LOCATION = \"europe-west2\"\n",
    "LOCATION_DEPLOY = \"europe-west2\" #Location to deploy GCP resources\n",
    "\n",
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55984b68-a0cd-4d7e-b3c0-9d5d3a274ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import ChatModel\n",
    "\n",
    "\"\"\" \n",
    "between using the most stable model in the last 6 month: chat-bison@001 \n",
    "and the latest, most capable model, there is a massive difference in \n",
    "terms of context awareness, length of answer, ability to follow instructions,\n",
    "etc. How do we measure these using metrics? I'm working on it.\n",
    "\n",
    "I lied, the latest model, whilst sometimes its answers can be amazing,\n",
    "definitely not as good when using RAG, it ignores context too often,\n",
    "until I fix that, we'll work with the stable version.\n",
    "\n",
    "\"\"\"\n",
    "class PaLMWrapper:\n",
    "    def __init__(self):\n",
    "        self.chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
    "        self.parameters = {\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_output_tokens\": 256,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "    \n",
    "    def generate_response(self, context, message):\n",
    "        chat = self.chat_model.start_chat(context=context)\n",
    "        response = chat.send_message(message, **self.parameters)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87611f14-b46d-468a-97f8-7c469b96b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = PaLMWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df757677-25f2-437b-a38e-76deca85e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "class VertexAIVectorStore:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # god awful way of doing it, should be a config and passed through but oh well hacky hack\n",
    "        self.gen_ai_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "            index_endpoint_name=\"projects/playpen-8d8611/locations/europe-west2/indexEndpoints/5891412000042385408\"\n",
    "        )\n",
    "\n",
    "        self.gen_ai_index = aiplatform.MatchingEngineIndex(\n",
    "            index_name=\"projects/playpen-8d8611/locations/europe-west2/indexes/2680908415680643072\"\n",
    "        )\n",
    "        \n",
    "        self.model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "        self.df = pd.read_csv('text_data_g_embedding.csv')\n",
    "        \n",
    "    def search(self, input, k=3):\n",
    "        embedding_vec =  self.model.get_embeddings([input])[0].values #Send request to embedding model to generate the embedding vector\n",
    "\n",
    "        #find neighbours using vector search\n",
    "        neighbours = self.gen_ai_index_endpoint.find_neighbors(\n",
    "            deployed_index_id=\"gen_ai_deployed_index\",\n",
    "            queries=[embedding_vec],\n",
    "            num_neighbors=k,\n",
    "        )[0]\n",
    "        \n",
    "        results = []\n",
    "        for nb in neighbours:\n",
    "            nb_id = int(nb.id)\n",
    "            if nb_id < len(self.df):\n",
    "                url = self.df.iloc[int(nb.id)]['url']\n",
    "                text = self.df.iloc[int(nb.id)]['text']\n",
    "                score = nb.distance\n",
    "                results.append((url, text, score))\n",
    "            else:\n",
    "                results=[('', '', 0)]\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8bf494d-8103-47d7-afa6-10555b5e422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vai = VertexAIVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c06457-c547-443b-b4dc-863c1e882ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couldnt get langchain to work so custom RAG anyone?\n",
    "import time\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, vector_store, palm_wrapper, initial_system_prompt=True):\n",
    "        self.vector_store = vector_store\n",
    "        self.palm_wrapper = palm_wrapper\n",
    "        # the comment from mathew regarding memory\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        if initial_system_prompt:            \n",
    "            system_prompt = r\"You are a professional assistant with extensive experience helping numerous small and medium businesses. You work for a large retail bank called Lloyds. Please assist the user answering questions with detailed responses, providing reasoning whenever prescriptive advice is given. Ensure your answers are elaborate and helpful.\"\n",
    "            self.conversation_history.append((system_prompt, \"\"))\n",
    "        \n",
    "    \n",
    "    def generate(self, query, is_user_query=True, k=3):\n",
    "        vector_start_time = time.time()\n",
    "        \n",
    "        contexts=[]\n",
    "        sources=[]\n",
    "        scores=[]\n",
    "        \n",
    "        if is_user_query:\n",
    "            # if user query, perform vector db search\n",
    "            # retrieve contexts based on the query\n",
    "            search_results = self.vector_store.search(query, k)\n",
    "            contexts = [str(result[1]) for result in search_results]\n",
    "            sources = [result[0] for result in search_results]\n",
    "            scores = [result[2] for result in search_results]\n",
    "            \n",
    "            if sum(scores)/len(scores) < 0.65:\n",
    "                for i in range(len(contexts)):\n",
    "                    \n",
    "                    contexts[i]='In your search you could not find relevant context within the dataset. Therefore, you are uncertain about your response. IMPORTANT: Remind the customer that you are built to answer Lloyds banking related questions only.'\n",
    "                    sources[i]='Could not find relevant sources'\n",
    "            \n",
    "              \n",
    "        vector_end_time = time.time()\n",
    "        # combine both query and response from conversation history\n",
    "        history_context = '\\n'.join(['Q: ' + query + '\\nA: ' + response for query, response in self.conversation_history[-k:]])\n",
    "        \n",
    "        # combine history context and current contexts\n",
    "        combined_context = '\\n'.join([history_context] + contexts)\n",
    "        combined_context = combined_context[-20000:]\n",
    "        \n",
    "        lm_start_time = time.time()\n",
    "        response = self.palm_wrapper.generate_response(combined_context, query)\n",
    "        lm_end_time = time.time()\n",
    "        # update conversation history with current interaction\n",
    "        self.conversation_history.append((query, response))\n",
    "        \n",
    "        vector_search_time = vector_end_time - vector_start_time\n",
    "        lm_inference_time = lm_end_time - lm_start_time\n",
    "        \n",
    "        return response, sources, scores, vector_search_time, lm_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f06b42-1f61-43dd-b7a2-86bf4c6d7670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that you've lost your card. Please call us right away on 0800 096 4496. We'll need to cancel your card and issue you with a new one.\n",
      "sources used: ['https://www.lloydsbank.com/business/help-and-support/cards/apply-for-a-business-debit-card.html', 'https://www.lloydsbank.com/business/help-and-support/security-and-fraud/report-lost-stolen-corporate-card.html', 'https://www.lloydsbank.com/business/help-and-support/online-banking/log-on-to-o4b.html?WT.ac=Lloyds-help_and_support-online_banking-button_text-log_on_to_online_for_business']\n",
      "scores: [0.7142672538757324, 0.7047110795974731, 0.7031532526016235]\n",
      "vector search time: 0.19745802879333496\n",
      "llm inference time: 0.6073603630065918\n",
      "This is to protect your account from fraud. If you have lost your card, it is important to cancel it as soon as possible so that no one else can use it. We will then issue you with a new card.\n",
      "sources used: ['https://www.lloydsbank.com/business/help-and-support/security-and-fraud/report-lost-stolen-business-card.html', 'https://www.lloydsbank.com/business/help-and-support/payments/how-to-stop-a-cheque.html?Wt.ac=help_and_support-payments-button_text-how_to_stop_a_cheque', 'https://www.lloydsbank.com/business/help-and-support/cards/apply-for-a-business-cashpoint-card.html']\n",
      "scores: [0.6711143851280212, 0.6682686805725098, 0.6521599292755127]\n",
      "vector search time: 0.17327046394348145\n",
      "llm inference time: 1.5494906902313232\n",
      "I'm sorry, I misunderstood. What can I help you with today?\n",
      "sources used: ['Could not find relevant sources']\n",
      "scores: [0.6711143851280212, 0.6682686805725098, 0.6521599292755127]\n",
      "vector search time: 0.4224221706390381\n",
      "llm inference time: 1.5494906902313232\n",
      "I'm sorry, I can't answer that. I'm built to answer Lloyds banking related questions only.\n",
      "sources used: ['Could not find relevant sources', 'Could not find relevant sources', 'Could not find relevant sources']\n",
      "scores: [0.6232798099517822, 0.5904541611671448, 0.5719844102859497]\n",
      "vector search time: 0.18743348121643066\n",
      "llm inference time: 0.43079447746276855\n",
      "To set up an online banking account, you will need to have a Lloyds Bank business current account. You can then log in to Online for Business and follow the instructions on screen.\n",
      "sources used: ['https://www.lloydsbank.com/business/help-and-support/online-banking/forgotten-o4b-logon-details.html?WT.ac=lloyds-bb-help_support-category-onlbnkg-text-forgotten-O4B-logon-details', 'https://www.lloydsbank.com/business/help-and-support/account-management/text-alerts.html?WT.ac=Lloyds-help_and_support-text_alerts-button_text-text_alerts', 'https://www.lloydsbank.com/business/help-and-support/contact-us.html?Wt.ac=Lloyds-footer-inline_link-contact_us']\n",
      "scores: [0.7497604489326477, 0.7378867864608765, 0.7196010947227478]\n",
      "vector search time: 0.16959285736083984\n",
      "llm inference time: 0.5557427406311035\n"
     ]
    }
   ],
   "source": [
    "rag = RAG(vai, pw)\n",
    "query = \"i've lost my card\"\n",
    "response, sources, scores, vector_search_time, lm_inference_time = rag.generate(query)\n",
    "print(response)\n",
    "print('sources used:', sources)\n",
    "print('scores:', scores)\n",
    "print('vector search time:', vector_search_time)\n",
    "print('llm inference time:', lm_inference_time)\n",
    "\n",
    "query = \"why?\"\n",
    "response, sources, scores, vector_search_time, lm_inference_time = rag.generate(query)\n",
    "print(response)\n",
    "print('sources used:', sources)\n",
    "print('scores:', scores)\n",
    "print('vector search time:', vector_search_time)\n",
    "print('llm inference time:', lm_inference_time)\n",
    "\n",
    "query = \"i was not asking about an audit request\"\n",
    "response, sources, score, vector_search_time, lm_inference_times = rag.generate(query)\n",
    "print(response)\n",
    "print('sources used:', sources)\n",
    "print('scores:', scores)\n",
    "print('vector search time:', vector_search_time)\n",
    "print('llm inference time:', lm_inference_time)\n",
    "\n",
    "query = \"what is the weather in london\"\n",
    "response, sources, scores, vector_search_time, lm_inference_time = rag.generate(query)\n",
    "print(response)\n",
    "print('sources used:', sources)\n",
    "print('scores:', scores)\n",
    "print('vector search time:', vector_search_time)\n",
    "print('llm inference time:', lm_inference_time)\n",
    "\n",
    "query = \"how can i set up an online banking account?\"\n",
    "response, sources, scores, vector_search_time, lm_inference_time = rag.generate(query)\n",
    "print(response)\n",
    "print('sources used:', sources)\n",
    "print('scores:', scores)\n",
    "print('vector search time:', vector_search_time)\n",
    "print('llm inference time:', lm_inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b7298fa-3852-4046-8a45-41e14ab1c7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c2a1d3e99b4fc78eb5c45aec0a7c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7572df24948d4453960ed14bd72e1e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Lloyds', layout=Layout(width='80%'), placeholder='Message LBG help …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUI:\n",
    "    def __init__(self, rag_instance):\n",
    "        self.rag_instance = rag_instance\n",
    "        self.conversation = []\n",
    "        self._setup_ui()\n",
    "    \n",
    "    def _setup_ui(self):\n",
    "        self.input_box = widgets.Text(\n",
    "            placeholder='Message LBG help and support...',\n",
    "            description='Lloyds',\n",
    "            layout={'width': '80%'}\n",
    "        )\n",
    "        \n",
    "        self.send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='info',\n",
    "            layout={'width': '12%'}\n",
    "        )\n",
    "        self.output_area = widgets.Output(layout={'border': '1px solid black', 'width': '100%'})\n",
    "        self.send_button.on_click(self._on_send_clicked)\n",
    "        \n",
    "        input_send_box = widgets.HBox([self.input_box, self.send_button])\n",
    "        \n",
    "        display(self.output_area, input_send_box)\n",
    "        \n",
    "    def _on_send_clicked(self, b):\n",
    "        query = self.input_box.value\n",
    "        self.conversation.append(f\"You: {query}\")\n",
    "        response = self.rag_instance.generate(query)\n",
    "        self.conversation.append(f'LBG AI: {response}')\n",
    "        \n",
    "        with self.output_area:\n",
    "            clear_output(wait=True)\n",
    "            print('\\n'.join(self.conversation))\n",
    "        \n",
    "        # clear input box\n",
    "        self.input_box.value = ''\n",
    "        \n",
    "rag = RAG(vai, pw)\n",
    "chat_ui = ChatUI(rag)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482d7de-97fb-42c0-b2ec-dfa6ced835be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
