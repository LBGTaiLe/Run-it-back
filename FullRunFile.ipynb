{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76588577-7c47-4730-815b-383b58f30130",
   "metadata": {},
   "source": [
    "## Important to install torch==1.12.0 and then to restart.\n",
    "If any problems occur, to do with libcuda, its most likely because torch version is higher than 1.12.0.\n",
    "\n",
    "This was not done via requirements - I've been trying, but running out of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473dbc8f-4b37-4f3e-af82-cbe4838a4d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.12.0 in /opt/conda/lib/python3.7/site-packages (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.0) (4.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fe5b7-fcb2-4578-90f9-bc3b5bb5b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b1539-d235-49d5-8869-47cd36267903",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'output'\n",
    "data = {'file_name' : [], 'text': []}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_data = file.read().replace('\\n', '')\n",
    "        \n",
    "        data['file_name'].append(filename)\n",
    "        data['text'].append(text_data)\n",
    "\n",
    "text_data_df = pd.DataFrame(data)\n",
    "text_data_df.to_csv('text_data.csv', index=False)\n",
    "\n",
    "print(text_data_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba8cd7-6de9-4f9b-9d1f-77633983c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install \"google-cloud-aiplatform>=1.25\" \"shapely<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71959bad-f5dd-4556-bf1d-2fd383222ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Vertex AI SDK\n",
    "PROJECT_ID = !gcloud config get project\n",
    "PROJECT_ID = PROJECT_ID.n\n",
    "LOCATION = \"europe-west2\"\n",
    "LOCATION_DEPLOY = \"europe-west2\" #Location to deploy GCP resources\n",
    "\n",
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708d96e-d84f-4e5d-aa51-a0ace46fb60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0eeb6b-f2d1-4a28-a8bb-b716bf58b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354ffe1-6f6b-49e2-9c60-01ed4fd3eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5b083-484b-4b6a-a173-b49b8673fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p datasets\n",
    "!gsutil copy datasets/vector_search_dataset.json gs://gen-ai-{PROJECT_ID}-bucket/embeddings/vs_root/vector_search_dataset.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157ad3c-836e-4a6d-8f71-1e56d984c4b8",
   "metadata": {},
   "source": [
    "## Generating API endpoints for yourself - This may take up to 30 minutes.\n",
    "\n",
    "This is only necessary on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6c111-9d60-461e-9fd9-4cad10c4aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = 768\n",
    "GS_URI = \"gs://gen-ai-%s-bucket/embeddings/vs_root/\" % PROJECT_ID\n",
    "\n",
    "gen_ai_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=\"Gen AI Index\",\n",
    "    contents_delta_uri=GS_URI,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=5,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    leaf_node_embedding_count=10,\n",
    "    leaf_nodes_to_search_percent=80,\n",
    "    description=\"Example Index for Gen AI Playpen\",\n",
    "    location=LOCATION_DEPLOY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47974b-13e0-49db-b0cf-93fbe6aa144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ai_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=\"Gen AI Index Endpoint\",\n",
    "    description=\"Example Index for Gen AI Playpen\",\n",
    "    public_endpoint_enabled=True,\n",
    "    location=LOCATION_DEPLOY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c780b70-4a83-4570-9226-207b6db87440",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ai_index_endpoint = gen_ai_index_endpoint.deploy_index(\n",
    "    index=gen_ai_index, deployed_index_id=\"gen_ai_deployed_index\",\n",
    "    machine_type=\"e2-standard-16\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1\n",
    ")\n",
    "\n",
    "gen_ai_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414202c-e26c-452a-80f3-c3c081273375",
   "metadata": {},
   "source": [
    "## Important\n",
    "Please go to google cloud. Go to dashboard and find project ID.\n",
    "Replace in the two: playpen-8d8611 -> your project ID\n",
    "index_name=\"projects/playpen-8d8611/locations/europe-west2/indexes/2680908415680643072\"\n",
    "index_endpoint_name=\"projects/playpen-8d8611/locations/europe-west2/indexEndpoints/5891412000042385408\"\n",
    "\n",
    "Then search for vector search. There should be an index and an indexendpoint tab. Go to index. Get ID. Replace: 2680908415680643072 -> your ID\n",
    "index_name=\"projects/playpen-8d8611/locations/europe-west2/indexes/2680908415680643072\"\n",
    "\n",
    "Go to indexEndpoint. Get ID.\n",
    "Replace: 5891412000042385408 -> your ID\n",
    "index_endpoint_name=\"projects/playpen-8d8611/locations/europe-west2/indexEndpoints/5891412000042385408\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac3f00-2aed-4a9e-9aa1-e6eb5b78d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ai_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "    index_endpoint_name=\"projects/playpen-8d8611/locations/europe-west2/indexEndpoints/5891412000042385408\"\n",
    ")\n",
    "\n",
    "gen_ai_index = aiplatform.MatchingEngineIndex(\n",
    "    index_name=\"projects/playpen-8d8611/locations/europe-west2/indexes/2680908415680643072\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7b759-bf08-446d-9869-831d1f7146b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import ChatModel\n",
    "\n",
    "\"\"\" \n",
    "between using the most stable model in the last 6 month: chat-bison@001 \n",
    "and the latest, most capable model, there is a massive difference in \n",
    "terms of context awareness, length of answer, ability to follow instructions,\n",
    "etc. How do we measure these using metrics? I'm working on it.\n",
    "\n",
    "I lied, the latest model, whilst sometimes its answers can be amazing,\n",
    "definitely not as good when using RAG, it ignores context too often,\n",
    "until I fix that, we'll work with the stable version.\n",
    "\n",
    "\"\"\"\n",
    "class PaLMWrapper:\n",
    "    def __init__(self):\n",
    "        self.chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
    "        self.parameters = {\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_output_tokens\": 256,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "    \n",
    "    def generate_response(self, context, message):\n",
    "        chat = self.chat_model.start_chat(context=context)\n",
    "        response = chat.send_message(message, **self.parameters)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad89e77-7822-4a8b-bb4d-0862824d13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = PaLMWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c3bff-64d6-4492-b0fb-5088e464732e",
   "metadata": {},
   "source": [
    "## Replace the endpoints with your project id, index id, and indexendpoint id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c856f6-2e6c-4693-997e-355d0b9521bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "class VertexAIVectorStore:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # god awful way of doing it, should be a config and passed through but oh well hacky hack\n",
    "        self.gen_ai_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "            index_endpoint_name=\"projects/playpen-8d8611/locations/europe-west2/indexEndpoints/8762456762491076608\"\n",
    "        )\n",
    "\n",
    "        self.gen_ai_index = aiplatform.MatchingEngineIndex(\n",
    "            index_name=\"projects/playpen-8d8611/locations/europe-west2/indexes/3025433787174486016\"\n",
    "        )\n",
    "        \n",
    "        self.model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "        self.df = pd.read_csv('text_data_g_embedding.csv')\n",
    "        \n",
    "    def search(self, input, k=3):\n",
    "        embedding_vec =  self.model.get_embeddings([input])[0].values #Send request to embedding model to generate the embedding vector\n",
    "\n",
    "        #find neighbours using vector search\n",
    "        neighbours = self.gen_ai_index_endpoint.find_neighbors(\n",
    "            deployed_index_id=\"gen_ai_deployed_index\",\n",
    "            queries=[embedding_vec],\n",
    "            num_neighbors=k,\n",
    "        )[0]\n",
    "        \n",
    "        results = []\n",
    "        for nb in neighbours:\n",
    "            nb_id = int(nb.id)\n",
    "            if nb_id < len(self.df):\n",
    "                url = self.df.iloc[int(nb.id)]['url']\n",
    "                text = self.df.iloc[int(nb.id)]['text']\n",
    "                score = nb.distance\n",
    "                results.append((url, text, score))\n",
    "            else:\n",
    "                results=[('', '', 0)]\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae540a8-07f1-4716-b61e-b53cec22f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vai = VertexAIVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dab35-322a-493e-941b-443c50879175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, vector_store, palm_wrapper, initial_system_prompt=True):\n",
    "        self.vector_store = vector_store\n",
    "        self.palm_wrapper = palm_wrapper\n",
    "        # the comment from mathew regarding memory\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        if initial_system_prompt:            \n",
    "            system_prompt = r\"You are a professional assistant with extensive experience helping numerous small and medium businesses. You work for a large retail bank called Lloyds. Please assist the user answering questions with detailed responses, providing reasoning whenever prescriptive advice is given. Ensure your answers are elaborate and helpful.\"\n",
    "            self.conversation_history.append((system_prompt, \"\"))\n",
    "        \n",
    "    \n",
    "    def generate(self, query, is_user_query=True, k=3):\n",
    "        vector_start_time = time.time()\n",
    "        \n",
    "        contexts=[]\n",
    "        sources=[]\n",
    "        scores=[]\n",
    "        \n",
    "        if is_user_query:\n",
    "            # if user query, perform vector db search\n",
    "            # retrieve contexts based on the query\n",
    "            search_results = self.vector_store.search(query, k)\n",
    "            contexts = [str(result[1]) for result in search_results]\n",
    "            sources = [result[0] for result in search_results]\n",
    "            scores = [result[2] for result in search_results]\n",
    "            \n",
    "            if sum(scores)/len(scores) < 0.65:\n",
    "                for i in range(len(contexts)):\n",
    "                    \n",
    "                    contexts[i]='In your search you could not find relevant context within the dataset. Therefore, you are uncertain about your response. IMPORTANT: Remind the customer that you are built to answer Lloyds banking related questions only.'\n",
    "                    sources[i]='Could not find relevant sources'\n",
    "            \n",
    "              \n",
    "        vector_end_time = time.time()\n",
    "        # combine both query and response from conversation history\n",
    "        history_context = '\\n'.join(['Q: ' + query + '\\nA: ' + response for query, response in self.conversation_history[-k:]])\n",
    "        \n",
    "        # combine history context and current contexts\n",
    "        combined_context = '\\n'.join([history_context] + contexts)\n",
    "        combined_context = combined_context[-20000:]\n",
    "        \n",
    "        lm_start_time = time.time()\n",
    "        response = self.palm_wrapper.generate_response(combined_context, query)\n",
    "        lm_end_time = time.time()\n",
    "        # update conversation history with current interaction\n",
    "        self.conversation_history.append((query, response))\n",
    "        \n",
    "        vector_search_time = vector_end_time - vector_start_time\n",
    "        lm_inference_time = lm_end_time - lm_start_time\n",
    "        \n",
    "        return response, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bf40c-2019-4a9b-828f-0db128e7772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUI:\n",
    "    def __init__(self, rag_instance):\n",
    "        self.rag_instance = rag_instance\n",
    "        self.conversation = []\n",
    "        self._setup_ui()\n",
    "    \n",
    "    def _setup_ui(self):\n",
    "        self.input_box = widgets.Text(\n",
    "            placeholder='Message LBG help and support...',\n",
    "            description='Lloyds',\n",
    "            layout={'width': '80%'}\n",
    "        )\n",
    "        \n",
    "        self.send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='info',\n",
    "            layout={'width': '12%'}\n",
    "        )\n",
    "        self.output_area = widgets.Output(layout={'border': '1px solid black', 'width': '100%'})\n",
    "        self.send_button.on_click(self._on_send_clicked)\n",
    "        \n",
    "        input_send_box = widgets.HBox([self.input_box, self.send_button])\n",
    "        \n",
    "        display(self.output_area, input_send_box)\n",
    "        \n",
    "    def _on_send_clicked(self, b):\n",
    "        query = self.input_box.value\n",
    "        self.conversation.append(f\"You: {query}\")\n",
    "        response = self.rag_instance.generate(query)\n",
    "        self.conversation.append(f'LBG AI: {response}')\n",
    "        \n",
    "        with self.output_area:\n",
    "            clear_output(wait=True)\n",
    "            print('\\n'.join(self.conversation))\n",
    "        \n",
    "        # clear input box\n",
    "        self.input_box.value = ''\n",
    "        \n",
    "rag = RAG(vai, pw)\n",
    "chat_ui = ChatUI(rag)    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
